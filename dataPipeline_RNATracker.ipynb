{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Genomics Project **RNA Localisation**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Problem definition**:\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inline installation instructions: \n",
    "# ! ONLY EXECUTE WHEN IN COLAB !\n",
    "#############################\n",
    "\n",
    "%pip install pandas\n",
    "%pip install torch\n",
    "%pip install numpy\n",
    "%pip install keras\n",
    "%pip install tensorflow\n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn\n",
    "%pip install pydot\n",
    "\n",
    "##############################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.** First steps with data\n",
    "\n",
    "Firstly, we import several necessary packages and load in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 13:37:18.598267: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-29 13:37:18.643922: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-29 13:37:18.645481: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-29 13:37:19.359413: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from keras.utils import to_categorical\n",
    "from keras.utils import to_categorical\n",
    "import itertools\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import sys\n",
    "#from keras.utils import pad_sequences\n",
    "from keras.utils import pad_sequences\n",
    "import tensorflow\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from dataloaders.GeneWrapper import Gene_Wrapper\n",
    "from sklearn.model_selection import train_test_split\n",
    "from plotting import plot_line_graph, box_plot\n",
    "from utils import read_model_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a few parameters\n",
    "max_len = 34526\n",
    "nb_filters = 32\n",
    "filters_length = 10\n",
    "pooling_size = 3\n",
    "lstm_units = 32\n",
    "lower_bound = 0\n",
    "upper_bound = 4000\n",
    "nb_classes = 9 # because we have 9 localisations\n",
    "batch_size = 256\n",
    "\n",
    "output_path = \"/outputs\" #\"~/Downloads/model_outputs\"\n",
    "model_architecture_path = \"model_architecture_viz/simple_cnn.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ERM</th>\n",
       "      <th>KDEL</th>\n",
       "      <th>LMA</th>\n",
       "      <th>MITO</th>\n",
       "      <th>NES</th>\n",
       "      <th>NIK</th>\n",
       "      <th>NLS</th>\n",
       "      <th>NUCP</th>\n",
       "      <th>OMM</th>\n",
       "      <th>gene_id</th>\n",
       "      <th>gene_biotype</th>\n",
       "      <th>seq</th>\n",
       "      <th>struct</th>\n",
       "      <th>m6A_5UTR</th>\n",
       "      <th>m6A_CDS</th>\n",
       "      <th>m6A_3UTR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57.045409</td>\n",
       "      <td>35.456782</td>\n",
       "      <td>22.008215</td>\n",
       "      <td>12.355106</td>\n",
       "      <td>22.789983</td>\n",
       "      <td>24.241731</td>\n",
       "      <td>16.970436</td>\n",
       "      <td>29.348389</td>\n",
       "      <td>54.916891</td>\n",
       "      <td>ENSG00000000003</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>ACCTTGTATTAGGTATTTATTTCCACAAAAGTTTGATGCTTACAAC...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.914814</td>\n",
       "      <td>0.244517</td>\n",
       "      <td>ENSG00000000005</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>TGTGCACAGAAGTTATATACATATATGGGTATATCTATGTAACAAA...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.449430</td>\n",
       "      <td>34.151539</td>\n",
       "      <td>29.338431</td>\n",
       "      <td>22.237585</td>\n",
       "      <td>24.682767</td>\n",
       "      <td>43.612551</td>\n",
       "      <td>38.683963</td>\n",
       "      <td>35.678476</td>\n",
       "      <td>25.348560</td>\n",
       "      <td>ENSG00000000419</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>TACTTTATGCAAAAAAAAATATACATTTATTTATAGGTCTCAATAC...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.830180</td>\n",
       "      <td>2.576734</td>\n",
       "      <td>5.737850</td>\n",
       "      <td>0.761343</td>\n",
       "      <td>2.786808</td>\n",
       "      <td>2.784356</td>\n",
       "      <td>3.382682</td>\n",
       "      <td>2.463676</td>\n",
       "      <td>2.819269</td>\n",
       "      <td>ENSG00000000457</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>TGACTTTCAAACCATTTTAATATTTCAAATATTCCAGAACAATCCC...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.132915</td>\n",
       "      <td>8.782925</td>\n",
       "      <td>10.061390</td>\n",
       "      <td>3.012459</td>\n",
       "      <td>8.821250</td>\n",
       "      <td>6.721117</td>\n",
       "      <td>10.827253</td>\n",
       "      <td>8.005113</td>\n",
       "      <td>6.849962</td>\n",
       "      <td>ENSG00000000460</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>AACCCGCTCGGGTCCCCTTCCACACTGTGGAAGCTTTGTTCTTTCG...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13805</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.506809</td>\n",
       "      <td>0.075893</td>\n",
       "      <td>0.050764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ENSG00000281883</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>GGGAAGAAAGGAGCCTGACTCTTATGATGGAATAACCACAAATCAG...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13806</th>\n",
       "      <td>0.105452</td>\n",
       "      <td>0.087130</td>\n",
       "      <td>0.171187</td>\n",
       "      <td>0.016101</td>\n",
       "      <td>0.079057</td>\n",
       "      <td>0.669947</td>\n",
       "      <td>0.171672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254546</td>\n",
       "      <td>ENSG00000282034</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>GTGTCGGACGGCATGACAGGCAGCAATCCTGTGTCCCCTGCCTCAT...</td>\n",
       "      <td>[0.37599998712539673, 0.0, 0.07500000298023224...</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13807</th>\n",
       "      <td>0.037093</td>\n",
       "      <td>0.444844</td>\n",
       "      <td>0.425368</td>\n",
       "      <td>0.254467</td>\n",
       "      <td>0.323794</td>\n",
       "      <td>1.803249</td>\n",
       "      <td>1.435483</td>\n",
       "      <td>0.249590</td>\n",
       "      <td>0.483645</td>\n",
       "      <td>ENSG00000282827</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>ATGGCGGGGACCTCCGCGCCAGGCAGCAAGAGGCGGAGCGAGCCCC...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13808</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519421</td>\n",
       "      <td>2.119115</td>\n",
       "      <td>0.360450</td>\n",
       "      <td>0.887939</td>\n",
       "      <td>0.345021</td>\n",
       "      <td>1.274465</td>\n",
       "      <td>0.741954</td>\n",
       "      <td>0.460649</td>\n",
       "      <td>ENSG00000282936</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>CTACTGGAACGCCCCCCTCAATCTAGCCTCCCCCACATAACTCTCT...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13809</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.226187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032740</td>\n",
       "      <td>0.157772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016665</td>\n",
       "      <td>ENSG00000282988</td>\n",
       "      <td>protein_coding</td>\n",
       "      <td>TGGTCTATCTCCCAAGTCAAAGACTGAAGTAAAGAAAAAGGTTGCA...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13810 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ERM       KDEL        LMA       MITO        NES        NIK  \\\n",
       "0      57.045409  35.456782  22.008215  12.355106  22.789983  24.241731   \n",
       "1       0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2      17.449430  34.151539  29.338431  22.237585  24.682767  43.612551   \n",
       "3       3.830180   2.576734   5.737850   0.761343   2.786808   2.784356   \n",
       "4      13.132915   8.782925  10.061390   3.012459   8.821250   6.721117   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "13805   0.000000   0.000000   0.506809   0.075893   0.050764   0.000000   \n",
       "13806   0.105452   0.087130   0.171187   0.016101   0.079057   0.669947   \n",
       "13807   0.037093   0.444844   0.425368   0.254467   0.323794   1.803249   \n",
       "13808   0.000000   0.519421   2.119115   0.360450   0.887939   0.345021   \n",
       "13809   0.000000   0.226187   0.000000   0.032740   0.157772   0.000000   \n",
       "\n",
       "             NLS       NUCP        OMM          gene_id    gene_biotype  \\\n",
       "0      16.970436  29.348389  54.916891  ENSG00000000003  protein_coding   \n",
       "1       0.000000   2.914814   0.244517  ENSG00000000005  protein_coding   \n",
       "2      38.683963  35.678476  25.348560  ENSG00000000419  protein_coding   \n",
       "3       3.382682   2.463676   2.819269  ENSG00000000457  protein_coding   \n",
       "4      10.827253   8.005113   6.849962  ENSG00000000460  protein_coding   \n",
       "...          ...        ...        ...              ...             ...   \n",
       "13805   0.056586   0.000000   0.000000  ENSG00000281883  protein_coding   \n",
       "13806   0.171672   0.000000   0.254546  ENSG00000282034  protein_coding   \n",
       "13807   1.435483   0.249590   0.483645  ENSG00000282827  protein_coding   \n",
       "13808   1.274465   0.741954   0.460649  ENSG00000282936  protein_coding   \n",
       "13809   0.301875   0.000000   0.016665  ENSG00000282988  protein_coding   \n",
       "\n",
       "                                                     seq  \\\n",
       "0      ACCTTGTATTAGGTATTTATTTCCACAAAAGTTTGATGCTTACAAC...   \n",
       "1      TGTGCACAGAAGTTATATACATATATGGGTATATCTATGTAACAAA...   \n",
       "2      TACTTTATGCAAAAAAAAATATACATTTATTTATAGGTCTCAATAC...   \n",
       "3      TGACTTTCAAACCATTTTAATATTTCAAATATTCCAGAACAATCCC...   \n",
       "4      AACCCGCTCGGGTCCCCTTCCACACTGTGGAAGCTTTGTTCTTTCG...   \n",
       "...                                                  ...   \n",
       "13805  GGGAAGAAAGGAGCCTGACTCTTATGATGGAATAACCACAAATCAG...   \n",
       "13806  GTGTCGGACGGCATGACAGGCAGCAATCCTGTGTCCCCTGCCTCAT...   \n",
       "13807  ATGGCGGGGACCTCCGCGCCAGGCAGCAAGAGGCGGAGCGAGCCCC...   \n",
       "13808  CTACTGGAACGCCCCCCTCAATCTAGCCTCCCCCACATAACTCTCT...   \n",
       "13809  TGGTCTATCTCCCAAGTCAAAGACTGAAGTAAAGAAAAAGGTTGCA...   \n",
       "\n",
       "                                                  struct  m6A_5UTR  m6A_CDS  \\\n",
       "0      [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...         0        0   \n",
       "1      [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...         0        0   \n",
       "2      [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...         0        0   \n",
       "3      [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...         0        9   \n",
       "4      [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...         0        0   \n",
       "...                                                  ...       ...      ...   \n",
       "13805  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...         0        2   \n",
       "13806  [0.37599998712539673, 0.0, 0.07500000298023224...         0       66   \n",
       "13807  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...         0        0   \n",
       "13808  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...         0        0   \n",
       "13809  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...         0        0   \n",
       "\n",
       "       m6A_3UTR  \n",
       "0             1  \n",
       "1             0  \n",
       "2             0  \n",
       "3             5  \n",
       "4             2  \n",
       "...         ...  \n",
       "13805         1  \n",
       "13806         4  \n",
       "13807         0  \n",
       "13808         2  \n",
       "13809         0  \n",
       "\n",
       "[13810 rows x 16 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing test set\n",
    "\n",
    "colab = False  #### Set colab flag ####\n",
    "\n",
    "if colab:\n",
    "    np.random.seed(3)\n",
    "    url = 'https://www.dropbox.com/s/hv4uau8q4wwg00k/final_data.csv?dl=1'\n",
    "    data_org = pd.read_csv(url)\n",
    "    test_data = data_org.sample(frac=0.1)\n",
    "    train_data = data_org.drop(test_data.index)\n",
    "else:\n",
    "    np.random.seed(3)\n",
    "    data_org = pd.read_csv('~/Downloads/final_data.csv')\n",
    "    test_data = data_org.sample(frac=0.1)\n",
    "    train_data = data_org.drop(test_data.index) # TODO: note: we also have to preprocess the test set similary\n",
    "    # TODO: colab\n",
    "\n",
    "data_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ERM</th>\n",
       "      <th>KDEL</th>\n",
       "      <th>LMA</th>\n",
       "      <th>MITO</th>\n",
       "      <th>NES</th>\n",
       "      <th>NIK</th>\n",
       "      <th>NLS</th>\n",
       "      <th>NUCP</th>\n",
       "      <th>OMM</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.207338</td>\n",
       "      <td>0.128871</td>\n",
       "      <td>0.079991</td>\n",
       "      <td>0.044906</td>\n",
       "      <td>0.082833</td>\n",
       "      <td>0.088109</td>\n",
       "      <td>0.061681</td>\n",
       "      <td>0.106670</td>\n",
       "      <td>0.199601</td>\n",
       "      <td>ACCTTGTATTAGGTATTTATTTCCACAAAAGTTTGATGCTTACAAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.922605</td>\n",
       "      <td>0.077395</td>\n",
       "      <td>TGTGCACAGAAGTTATATACATATATGGGTATATCTATGTAACAAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.064346</td>\n",
       "      <td>0.125935</td>\n",
       "      <td>0.108187</td>\n",
       "      <td>0.082002</td>\n",
       "      <td>0.091019</td>\n",
       "      <td>0.160823</td>\n",
       "      <td>0.142649</td>\n",
       "      <td>0.131566</td>\n",
       "      <td>0.093474</td>\n",
       "      <td>TACTTTATGCAAAAAAAAATATACATTTATTTATAGGTCTCAATAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.141112</td>\n",
       "      <td>0.094932</td>\n",
       "      <td>0.211394</td>\n",
       "      <td>0.028049</td>\n",
       "      <td>0.102672</td>\n",
       "      <td>0.102581</td>\n",
       "      <td>0.124625</td>\n",
       "      <td>0.090767</td>\n",
       "      <td>0.103868</td>\n",
       "      <td>TGACTTTCAAACCATTTTAATATTTCAAATATTCCAGAACAATCCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.172315</td>\n",
       "      <td>0.115240</td>\n",
       "      <td>0.132014</td>\n",
       "      <td>0.039526</td>\n",
       "      <td>0.115743</td>\n",
       "      <td>0.088187</td>\n",
       "      <td>0.142063</td>\n",
       "      <td>0.105034</td>\n",
       "      <td>0.089878</td>\n",
       "      <td>AACCCGCTCGGGTCCCCTTCCACACTGTGGAAGCTTTGTTCTTTCG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13805</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.734451</td>\n",
       "      <td>0.109981</td>\n",
       "      <td>0.073566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>GGGAAGAAAGGAGCCTGACTCTTATGATGGAATAACCACAAATCAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13806</th>\n",
       "      <td>0.067811</td>\n",
       "      <td>0.056029</td>\n",
       "      <td>0.110082</td>\n",
       "      <td>0.010354</td>\n",
       "      <td>0.050838</td>\n",
       "      <td>0.430809</td>\n",
       "      <td>0.110393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163685</td>\n",
       "      <td>GTGTCGGACGGCATGACAGGCAGCAATCCTGTGTCCCCTGCCTCAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13807</th>\n",
       "      <td>0.006797</td>\n",
       "      <td>0.081510</td>\n",
       "      <td>0.077941</td>\n",
       "      <td>0.046627</td>\n",
       "      <td>0.059330</td>\n",
       "      <td>0.330415</td>\n",
       "      <td>0.263028</td>\n",
       "      <td>0.045733</td>\n",
       "      <td>0.088620</td>\n",
       "      <td>ATGGCGGGGACCTCCGCGCCAGGCAGCAAGAGGCGGAGCGAGCCCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13808</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077421</td>\n",
       "      <td>0.315861</td>\n",
       "      <td>0.053726</td>\n",
       "      <td>0.132350</td>\n",
       "      <td>0.051426</td>\n",
       "      <td>0.189963</td>\n",
       "      <td>0.110591</td>\n",
       "      <td>0.068661</td>\n",
       "      <td>CTACTGGAACGCCCCCCTCAATCTAGCCTCCCCCACATAACTCTCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13809</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044530</td>\n",
       "      <td>0.214586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.410580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022666</td>\n",
       "      <td>TGGTCTATCTCCCAAGTCAAAGACTGAAGTAAAGAAAAAGGTTGCA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12429 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ERM      KDEL       LMA      MITO       NES       NIK       NLS  \\\n",
       "0      0.207338  0.128871  0.079991  0.044906  0.082833  0.088109  0.061681   \n",
       "1      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2      0.064346  0.125935  0.108187  0.082002  0.091019  0.160823  0.142649   \n",
       "3      0.141112  0.094932  0.211394  0.028049  0.102672  0.102581  0.124625   \n",
       "4      0.172315  0.115240  0.132014  0.039526  0.115743  0.088187  0.142063   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "13805  0.000000  0.000000  0.734451  0.109981  0.073566  0.000000  0.082002   \n",
       "13806  0.067811  0.056029  0.110082  0.010354  0.050838  0.430809  0.110393   \n",
       "13807  0.006797  0.081510  0.077941  0.046627  0.059330  0.330415  0.263028   \n",
       "13808  0.000000  0.077421  0.315861  0.053726  0.132350  0.051426  0.189963   \n",
       "13809  0.000000  0.307638  0.000000  0.044530  0.214586  0.000000  0.410580   \n",
       "\n",
       "           NUCP       OMM                                                seq  \n",
       "0      0.106670  0.199601  ACCTTGTATTAGGTATTTATTTCCACAAAAGTTTGATGCTTACAAC...  \n",
       "1      0.922605  0.077395  TGTGCACAGAAGTTATATACATATATGGGTATATCTATGTAACAAA...  \n",
       "2      0.131566  0.093474  TACTTTATGCAAAAAAAAATATACATTTATTTATAGGTCTCAATAC...  \n",
       "3      0.090767  0.103868  TGACTTTCAAACCATTTTAATATTTCAAATATTCCAGAACAATCCC...  \n",
       "4      0.105034  0.089878  AACCCGCTCGGGTCCCCTTCCACACTGTGGAAGCTTTGTTCTTTCG...  \n",
       "...         ...       ...                                                ...  \n",
       "13805  0.000000  0.000000  GGGAAGAAAGGAGCCTGACTCTTATGATGGAATAACCACAAATCAG...  \n",
       "13806  0.000000  0.163685  GTGTCGGACGGCATGACAGGCAGCAATCCTGTGTCCCCTGCCTCAT...  \n",
       "13807  0.045733  0.088620  ATGGCGGGGACCTCCGCGCCAGGCAGCAAGAGGCGGAGCGAGCCCC...  \n",
       "13808  0.110591  0.068661  CTACTGGAACGCCCCCCTCAATCTAGCCTCCCCCACATAACTCTCT...  \n",
       "13809  0.000000  0.022666  TGGTCTATCTCCCAAGTCAAAGACTGAAGTAAAGAAAAAGGTTGCA...  \n",
       "\n",
       "[12429 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sum_vec = train_data.iloc[:, :9].sum(axis=1)\n",
    "data2 = train_data.iloc[:, :9].divide(sum_vec, axis='index')\n",
    "train_data_no_struct = pd.concat([data2, train_data['seq']], axis=1)\n",
    "train_data_no_struct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One hot encode the 'seq' attribute of the above table\n",
    "mapping = {\n",
    "    'A': 0,\n",
    "    'C': 1,\n",
    "    'G': 2,\n",
    "    'T': 3\n",
    "}\n",
    "\n",
    "mapping_localisations = {\n",
    "    'ERM':  0,\n",
    "    'KDEL': 1,\n",
    "    'LMA':  2,\n",
    "    'MITO': 3,\n",
    "    'NES':  4,\n",
    "    'NIK':  5,\n",
    "    'NLS':  6,\n",
    "    'NUCP': 7,\n",
    "    'OMM':  8\n",
    "}\n",
    "\n",
    "one_hot_encode_lam = lambda seq: to_categorical([mapping[x] for x in seq])\n",
    "data_one = train_data_no_struct['seq'].apply(one_hot_encode_lam)\n",
    "\n",
    "data_one\n",
    "\n",
    "# Now just injecting this modified 'seq' back into the pandas frame\n",
    "data_one_no_struct =  pd.concat([train_data_no_struct.iloc[:, :9], data_one], axis=1)\n",
    "\n",
    "data_one_no_struct\n",
    "\n",
    "\n",
    "# Additional ordinal encoding of the 'seq' attribute\n",
    "\n",
    "gene_data = train_data['seq']\n",
    "\n",
    "def label_dist(dist):\n",
    "    # TODO: what is this\n",
    "    assert (len(dist) == 4)\n",
    "    return np.array(dist) / np.sum(dist)\n",
    "\n",
    "encoding_seq = OrderedDict([\n",
    "    ('UNK', [0, 0, 0, 0]),\n",
    "    ('A', [1, 0, 0, 0]),\n",
    "    ('C', [0, 1, 0, 0]),\n",
    "    ('G', [0, 0, 1, 0]),\n",
    "    ('T', [0, 0, 0, 1]),\n",
    "    ('N', [0.25, 0.25, 0.25, 0.25]),  # A or C or G or T\n",
    "])\n",
    "\n",
    "encoding_keys = list(encoding_seq.keys())\n",
    "seq_encoding_vectors = np.array(list(encoding_seq.values()))\n",
    "encoding_vectors = seq_encoding_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ERM', 'KDEL', 'LMA', 'MITO', 'NES', 'NIK', 'NLS', 'NUCP', 'OMM'])\n",
      "['ERM', 'KDEL', 'LMA', 'MITO', 'NES', 'NIK', 'NLS', 'NUCP', 'OMM']\n",
      "            ERM      KDEL       LMA      MITO       NES       NIK       NLS  \\\n",
      "0      0.207338  0.128871  0.079991  0.044906  0.082833  0.088109  0.061681   \n",
      "1      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "2      0.064346  0.125935  0.108187  0.082002  0.091019  0.160823  0.142649   \n",
      "3      0.141112  0.094932  0.211394  0.028049  0.102672  0.102581  0.124625   \n",
      "4      0.172315  0.115240  0.132014  0.039526  0.115743  0.088187  0.142063   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "13805  0.000000  0.000000  0.734451  0.109981  0.073566  0.000000  0.082002   \n",
      "13806  0.067811  0.056029  0.110082  0.010354  0.050838  0.430809  0.110393   \n",
      "13807  0.006797  0.081510  0.077941  0.046627  0.059330  0.330415  0.263028   \n",
      "13808  0.000000  0.077421  0.315861  0.053726  0.132350  0.051426  0.189963   \n",
      "13809  0.000000  0.307638  0.000000  0.044530  0.214586  0.000000  0.410580   \n",
      "\n",
      "           NUCP       OMM  \n",
      "0      0.106670  0.199601  \n",
      "1      0.922605  0.077395  \n",
      "2      0.131566  0.093474  \n",
      "3      0.090767  0.103868  \n",
      "4      0.105034  0.089878  \n",
      "...         ...       ...  \n",
      "13805  0.000000  0.000000  \n",
      "13806  0.000000  0.163685  \n",
      "13807  0.045733  0.088620  \n",
      "13808  0.110591  0.068661  \n",
      "13809  0.000000  0.022666  \n",
      "\n",
      "[12429 rows x 9 columns]\n",
      "[[0.20733762 0.12887145 0.0799912  ... 0.06168086 0.10666985 0.19960129]\n",
      " [0.         0.         0.         ... 0.         0.92260468 0.07739532]\n",
      " [0.06434552 0.12593526 0.10818672 ... 0.14264876 0.1315659  0.0934739 ]\n",
      " ...\n",
      " [0.00679663 0.08151006 0.07794138 ... 0.26302778 0.04573318 0.08861974]\n",
      " [0.         0.07742136 0.31586081 ... 0.18996313 0.11059064 0.06866123]\n",
      " [0.         0.307638   0.         ... 0.41058008 0.         0.02266626]]\n"
     ]
    }
   ],
   "source": [
    "#print(mapping_localisations.keys())\n",
    "#print(list(mapping_localisations.keys()))\n",
    "#print(data_one_no_struct[mapping_localisations.keys()])\n",
    "#print(data_one_no_struct[mapping_localisations.keys()].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNK', 'A', 'C', 'G', 'T', 'N']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = pad_sequences([[encoding_keys.index(c) for c in gene] for gene in gene_data],\n",
    "                    maxlen=max_len,\n",
    "                    dtype=np.int8, value=encoding_keys.index('UNK'))  # , truncating='post')\n",
    "\n",
    "y = data_one_no_struct[mapping_localisations.keys()].values\n",
    "\n",
    "# See notes to extend this for secondary structure\n",
    "\n",
    "encoding_keys\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2** |  Importing models...\n",
    "\n",
    "We use as baseline model the [RNATracker](https://github.com/HarveyYan/RNATracker/blob/master/Models/cnn_bilstm_attention.py) model and a CNN model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: ''\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 981,\n",
       " 982,\n",
       " 983,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 987,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999,\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import NN\n",
    "from models.RNAtracker import RNATracker\n",
    "\n",
    "# Set paths for model output\n",
    "try:\n",
    "    os.makedirs('')\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "\n",
    "model_output_folder = output_path #'~/Downloads/model_outputs'\n",
    "epochs = 15\n",
    "\n",
    "list_indizes = [i for i in range(X.shape[0])]\n",
    "list_indizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced Masking\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 34526)]      0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 34526, 4)     24          ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 34517, 32)    1280        ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 11505, 32)    0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 11505, 32)    0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 11496, 32)    10240       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 3832, 32)    0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 3832, 32)     0           ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " masking (Masking)              (None, 3832, 32)     0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 3832, 64)     16640       ['masking[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 3832, 64)     0           ['bidirectional[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3832, 50)     3250        ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3832, 1)      50          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 3832)         0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 3832)         0           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 3832, 1)      0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 3832, 64)     0           ['lambda[0][0]',                 \n",
      "                                                                  'reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 64)           0           ['multiply[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 9)            585         ['lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 32,069\n",
      "Trainable params: 32,045\n",
      "Non-trainable params: 24\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 13:40:24.580386: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 308938648 exceeds 10% of free system memory.\n",
      "2023-06-29 13:40:27.044291: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 141418496 exceeds 10% of free system memory.\n",
      "2023-06-29 13:40:27.059766: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1131053056 exceeds 10% of free system memory.\n",
      "2023-06-29 13:40:27.389236: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 376995840 exceeds 10% of free system memory.\n",
      "2023-06-29 13:40:27.560341: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 376700928 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 58s 2s/step - loss: 2.1975 - acc: 0.1134\n",
      "[2.197521448135376, 0.11343316733837128]\n",
      "4/4 [==============================] - 6s 1s/step - loss: 2.1975 - acc: 0.1035\n",
      "[2.1975295543670654, 0.10351758450269699]\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 13:41:30.665901: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-06-29 13:41:32.301947: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: with understand what we have to predict, we can allocate X and y\n",
    "# Also: the kwargsvalues are hyperparameters of which we will select default values from the RNAtracker repo\n",
    "\n",
    "\n",
    "train_indices, test_indices = train_test_split(list_indizes, random_state=42, test_size=0.2)\n",
    "\n",
    "model = RNATracker(max_len, nb_classes, model_output_folder, kfold_index=0)\n",
    "#model.build_model(nb_filters=kwargs['nb_filters'], filters_length=kwargs['filters_length'],\n",
    "#                          pooling_size=kwargs['pooling_size'], lstm_units=kwargs['lstm_units'],\n",
    "#                          embedding_vec=encoding_vectors)\n",
    "\n",
    "model.build_model_advanced_masking(nb_filters=nb_filters,\n",
    "                                                filters_length=filters_length,\n",
    "                                                pooling_size=pooling_size,\n",
    "                                                lstm_units=lstm_units,\n",
    "                                                embedding_vec=encoding_vectors)\n",
    "\n",
    "\n",
    "model.train(X[train_indices], y[train_indices], batch_size,epochs)\n",
    "\n",
    "\n",
    "\n",
    "history = model.stored_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = model.evaluate(X[test_indices], y[test_indices], \"\")\n",
    "print(str(a))\n",
    "model_output_path = os.path.join(model_output_folder, \"modelRNAtracker\")\n",
    "model.save(model_output_path)\n",
    "\n",
    "#results = model.evaluate(eval_data=train_data.iloc[valid_split], **param_dataLoader_valid)\n",
    "#results = dict(zip(model.model.metrics_names, results))\n",
    "\n",
    "#VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "#VALIDATION_LOSS.append(results['loss'])\n",
    "#K.clear_session()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.print_model(model_architecture_path)\n",
    "import datetime\n",
    "import pydot\n",
    "time_date = datetime.datetime.now().date()\n",
    "# time_hour = datetime.datetime.now().time()\n",
    "\n",
    "model_output = f\"model_outputs/simple_cnn_{time_date}.h5\"\n",
    "\n",
    "model.save_model(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_data = [history.history['loss'], history.history['val_loss']]\n",
    "plot_line_graph(plt_data, \"Loss Graph\", 'loss', 'epoch', ['train', 'val'])\n",
    "\n",
    "plt_data = [history.history['accuracy'], history.history['val_accuracy']]\n",
    "plot_line_graph(plt_data, \"Accuracy Graph\", 'accuracy', 'epoch', ['train', 'val'])\n",
    "\n",
    "plt_data = [history.history['tf_pearson'], history.history['val_tf_pearson']]\n",
    "plot_line_graph(plt_data, \"tf_pearson\", 'tf_pearson', 'epoch', ['train', 'val'])\n",
    "\n",
    "box_plot(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = model.evaluate(test_data, **param_dataLoader_valid)\n",
    "result = dict(zip(model.model.metrics_names, test_result))\n",
    "TEST_ACCURACY = result['accuracy']\n",
    "TEST_LOSS = result['loss']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rna_localization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
