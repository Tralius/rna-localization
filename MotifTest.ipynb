{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "# Modisco\n",
    "%matplotlib inline\n",
    "import modisco\n",
    "import json\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from metrics import Pearson\n",
    "from collections import OrderedDict, Counter, defaultdict\n",
    "from deeplift.dinuc_shuffle import dinuc_shuffle\n",
    "import random\n",
    "from random import shuffle\n",
    "from keras.models import load_model, model_from_json\n",
    "from keras.utils import to_categorical\n",
    "from utils import prepare_data\n",
    "from importlib import reload\n",
    "from deeplift.visualization import viz_sequence\n",
    "import shap\n",
    "import shap.explainers._deep.deep_tf\n",
    "reload(shap.explainers._deep.deep_tf)\n",
    "reload(shap.explainers._deep)\n",
    "reload(shap.explainers)\n",
    "reload(shap)\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T09:29:00.510707Z",
     "start_time": "2023-07-16T09:28:55.759900100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#compile the dinucleotide edges\n",
    "def prepare_edges(s):\n",
    " edges = defaultdict(list)\n",
    " for i in range(len(s)-1):\n",
    "     edges[s[i]].append(s[i+1])\n",
    " return edges\n",
    "\n",
    "\n",
    "def shuffle_edges(edges):\n",
    " #for each character, remove the last edge, shuffle, add edge back\n",
    " for char in edges:\n",
    "     last_edge = edges[char][-1]\n",
    "     edges[char] = edges[char][:-1]\n",
    "     the_list = edges[char]\n",
    "     shuffle(the_list)\n",
    "     edges[char].append(last_edge)\n",
    " return edges\n",
    "\n",
    "\n",
    "def traverse_edges(s, edges):\n",
    " generated = [s[0]]\n",
    " edges_queue_pointers = defaultdict(lambda: 0)\n",
    " for i in range(len(s)-1):\n",
    "     last_char = generated[-1]\n",
    "     generated.append(edges[last_char][edges_queue_pointers[last_char]])\n",
    "     edges_queue_pointers[last_char] += 1\n",
    " return \"\".join(generated)\n",
    "\n",
    "def one_hot_emb(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    mapping = {\n",
    "        'A': 0,\n",
    "        'C': 1,\n",
    "        'G': 2,\n",
    "        'T': 3\n",
    "    }\n",
    "    one_hot_encode_lam = lambda seq: to_categorical([mapping[x] for x in seq])\n",
    "    return data.apply(one_hot_encode_lam)\n",
    "\n",
    "def onehot_dinuc_shuffle(s):\n",
    "    s = np.squeeze(s)\n",
    "    argmax_vals = \"\".join([str(x) for x in np.argmax(s, axis=-1)])\n",
    "    shuffled_argmax_vals = [int(x) for x in traverse_edges(argmax_vals,\n",
    "                            shuffle_edges(prepare_edges(argmax_vals)))]\n",
    "    to_return = np.zeros_like(s)\n",
    "    to_return[list(range(len(s))), shuffled_argmax_vals] = 1\n",
    "    return to_return"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T09:29:02.441105800Z",
     "start_time": "2023-07-16T09:29:02.438101200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = prepare_data()\n",
    "\n",
    "max_seq_len = valid_data['seq'].apply(lambda x: len(x)).max()\n",
    "\n",
    "max_seq_len = 34526\n",
    "\n",
    "valid_data['length'] = valid_data.seq.str.len()\n",
    "\n",
    "valid_data = valid_data[valid_data.length <= max_seq_len]\n",
    "\n",
    "data_one_hot = one_hot_emb(valid_data['seq'])\n",
    "#data_struct = train_data['struct']\n",
    "\n",
    "#padded_sequences = np.zeros((len(train_data['seq']), max_seq_len, 6), dtype=np.float32)\n",
    "padded_sequences = np.zeros((len(valid_data['seq']), max_seq_len, 4), dtype=np.float32)\n",
    "\n",
    "indices = np.arange(valid_data.shape[0])\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    #tmp = data_struct.iloc[idx]\n",
    "    #padded_mask_struct = np.expand_dims(np.array(tmp != 'nan'), axis=1)\n",
    "    #tmp[tmp == 'nan'] = -1\n",
    "    #padded_struct = np.expand_dims(tmp, axis=1).astype('float64')\n",
    "    seq_data = data_one_hot.iloc[idx]\n",
    "    #seq_data = np.concatenate([seq_data, padded_mask_struct, padded_struct], axis=1)\n",
    "    padded_sequences[i, -len(data_one_hot.iloc[idx]):, :] = seq_data\n",
    "\n",
    "data_one_hot = padded_sequences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T09:29:15.196508600Z",
     "start_time": "2023-07-16T09:29:04.615779600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model_name = \"CNN_Baseline_Classic_2023-07-12\"\n",
    "\n",
    "model = load_model(f'model_outputs/{model_name}.h5', compile=False)\n",
    "\n",
    "#load the keras model\n",
    "keras_model_weights = f\"model_outputs/{model_name}_weights.h5\"\n",
    "keras_model_json = f\"model_outputs/{model_name}.json\"\n",
    "\n",
    "# serialize model to JSON\n",
    "if not os.path.isfile(keras_model_json):\n",
    "    model_json = model.to_json()\n",
    "    with open(keras_model_json, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(keras_model_weights)\n",
    "\n",
    "with open(keras_model_json, \"r\") as keras_model_json_file:\n",
    "    keras_model_as_json = json.load(keras_model_json_file)\n",
    "\n",
    "keras_model = model_from_json(open(keras_model_json).read())\n",
    "keras_model.load_weights(keras_model_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T09:30:21.107789100Z",
     "start_time": "2023-07-16T09:30:20.713096500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    DEEPEXPLAINER - SHAP\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "import sys\n",
    "\n",
    "shuffle_several_times = lambda s: np.array([onehot_dinuc_shuffle(s) for i in range(10)])\n",
    "seqs_to_explain = data_one_hot[0] #these three are positive for task 0\n",
    "\n",
    "sys.setrecursionlimit(5000)\n",
    "\n",
    "dinuc_shuff_explainer = shap.DeepExplainer((keras_model.input, keras_model.output[:,0]),\n",
    "                                           shuffle_several_times)\n",
    "raw_shap_explanations = dinuc_shuff_explainer.shap_values(seqs_to_explain)\n",
    "\n",
    "#project the importance at each position onto the base that's actually present\n",
    "shap_explanations = np.sum(raw_shap_explanations,axis=-1)[:,:,None]*seqs_to_explain\n",
    "\n",
    "for idx,(hypimpscores,orig_seq) in enumerate(zip(shap_explanations,seqs_to_explain)):\n",
    "    print(\"Sequence idx\",idx)\n",
    "    print(\"Actual contributions\")\n",
    "    # (The actual importance scores can be computed using an element-wise product of\n",
    "    #  the hypothetical importance scores and the actual importance scores)\n",
    "    viz_sequence.plot_weights(hypimpscores*orig_seq, subticks_frequency=20)\n",
    "    print(\"Hypothetical contributions\")\n",
    "    viz_sequence.plot_weights(hypimpscores, subticks_frequency=20)\n",
    "\n",
    "hypothetical_scores = shap_explanations\n",
    "original_scores = shap_explanations*seqs_to_explain"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-16T09:24:55.450813300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Don't know how to convert Functional",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdeeplift\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconversion\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkerasapi_conversion\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mkc\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OrderedDict\n\u001B[1;32m----> 8\u001B[0m deeplift_model \u001B[38;5;241m=\u001B[39m \u001B[43mkc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_model_from_saved_files\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m        \u001B[49m\u001B[43mh5_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeras_model_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m        \u001B[49m\u001B[43mjson_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeras_model_json\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\reggen\\Lib\\site-packages\\deeplift\\conversion\\kerasapi_conversion.py:379\u001B[0m, in \u001B[0;36mconvert_model_from_saved_files\u001B[1;34m(h5_file, json_file, yaml_file, **kwargs)\u001B[0m\n\u001B[0;32m    377\u001B[0m     model_conversion_function \u001B[38;5;241m=\u001B[39m convert_functional_model\n\u001B[0;32m    378\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 379\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDon\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt know how to convert \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    380\u001B[0m                               \u001B[38;5;241m+\u001B[39mmodel_class_name)\n\u001B[0;32m    382\u001B[0m \u001B[38;5;66;03m#add in the weights of the layer to the layer config\u001B[39;00m\n\u001B[0;32m    383\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer_config \u001B[38;5;129;01min\u001B[39;00m layer_configs:\n",
      "\u001B[1;31mNotImplementedError\u001B[0m: Don't know how to convert Functional"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    DEEPLIFT\n",
    "\"\"\"\n",
    "\n",
    "import deeplift.conversion.kerasapi_conversion as kc\n",
    "from collections import OrderedDict\n",
    "\n",
    "deeplift_model = kc.convert_model_from_saved_files(\n",
    "        h5_file=keras_model_weights,\n",
    "        json_file=keras_model_json)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T09:31:39.483899600Z",
     "start_time": "2023-07-16T09:31:39.456555Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMPQdX4cOoZzqVG2F9xuy/Y",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
