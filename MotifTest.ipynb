{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Modisco\n",
    "%matplotlib inline\n",
    "import modisco\n",
    "import json\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from metrics import Pearson\n",
    "from collections import OrderedDict, Counter, defaultdict\n",
    "from deeplift.dinuc_shuffle import dinuc_shuffle\n",
    "import random\n",
    "from random import shuffle\n",
    "from keras.models import load_model, model_from_json\n",
    "from keras.utils import to_categorical\n",
    "from utils import prepare_data\n",
    "from importlib import reload\n",
    "from deeplift.visualization import viz_sequence\n",
    "import shap\n",
    "import shap.explainers._deep.deep_tf\n",
    "reload(shap.explainers._deep.deep_tf)\n",
    "reload(shap.explainers._deep)\n",
    "reload(shap.explainers)\n",
    "reload(shap)\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#compile the dinucleotide edges\n",
    "def prepare_edges(s):\n",
    " edges = defaultdict(list)\n",
    " for i in range(len(s)-1):\n",
    "     edges[s[i]].append(s[i+1])\n",
    " return edges\n",
    "\n",
    "\n",
    "def shuffle_edges(edges):\n",
    " #for each character, remove the last edge, shuffle, add edge back\n",
    " for char in edges:\n",
    "     last_edge = edges[char][-1]\n",
    "     edges[char] = edges[char][:-1]\n",
    "     the_list = edges[char]\n",
    "     shuffle(the_list)\n",
    "     edges[char].append(last_edge)\n",
    " return edges\n",
    "\n",
    "\n",
    "def traverse_edges(s, edges):\n",
    " generated = [s[0]]\n",
    " edges_queue_pointers = defaultdict(lambda: 0)\n",
    " for i in range(len(s)-1):\n",
    "     last_char = generated[-1]\n",
    "     generated.append(edges[last_char][edges_queue_pointers[last_char]])\n",
    "     edges_queue_pointers[last_char] += 1\n",
    " return \"\".join(generated)\n",
    "\n",
    "def one_hot_emb(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    mapping = {\n",
    "        'A': 0,\n",
    "        'C': 1,\n",
    "        'G': 2,\n",
    "        'T': 3\n",
    "    }\n",
    "    one_hot_encode_lam = lambda seq: to_categorical([mapping[x] for x in seq])\n",
    "    return data.apply(one_hot_encode_lam)\n",
    "\n",
    "def onehot_dinuc_shuffle(s):\n",
    "    s = np.squeeze(s)\n",
    "    argmax_vals = \"\".join([str(x) for x in np.argmax(s, axis=-1)])\n",
    "    shuffled_argmax_vals = [int(x) for x in traverse_edges(argmax_vals,\n",
    "                            shuffle_edges(prepare_edges(argmax_vals)))]\n",
    "    to_return = np.zeros_like(s)\n",
    "    to_return[list(range(len(s))), shuffled_argmax_vals] = 1\n",
    "    return to_return"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T15:08:00.788116Z",
     "start_time": "2023-07-15T15:08:00.746763Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = prepare_data()\n",
    "\n",
    "max_seq_len = train_data['seq'].apply(lambda x: len(x)).max()\n",
    "\n",
    "max_seq_len = 20788\n",
    "\n",
    "train_data['length'] = train_data.seq.str.len()\n",
    "\n",
    "train_data = train_data[train_data.length <= max_seq_len]\n",
    "\n",
    "data_one_hot = one_hot_emb(train_data['seq'])\n",
    "#data_struct = train_data['struct']\n",
    "\n",
    "#padded_sequences = np.zeros((len(train_data['seq']), max_seq_len, 6), dtype=np.float32)\n",
    "padded_sequences = np.zeros((len(train_data['seq']), max_seq_len, 4), dtype=np.float32)\n",
    "\n",
    "indices = np.arange(train_data.shape[0])\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    #tmp = data_struct.iloc[idx]\n",
    "    #padded_mask_struct = np.expand_dims(np.array(tmp != 'nan'), axis=1)\n",
    "    #tmp[tmp == 'nan'] = -1\n",
    "    #padded_struct = np.expand_dims(tmp, axis=1).astype('float64')\n",
    "    seq_data = data_one_hot.iloc[idx]\n",
    "    #seq_data = np.concatenate([seq_data, padded_mask_struct, padded_struct], axis=1)\n",
    "    padded_sequences[i, -len(data_one_hot.iloc[idx]):, :] = seq_data\n",
    "\n",
    "data_one_hot = padded_sequences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T15:08:36.846576Z",
     "start_time": "2023-07-15T15:08:03.754769Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = load_model('model_outputs/moitfTest3.keras', compile=False)\n",
    "\n",
    "#load the keras model\n",
    "keras_model_weights = \"model_outputs/moitfTest3_weights.h5\"\n",
    "keras_model_json = \"model_outputs/moitfTest3.json\"\n",
    "\n",
    "# serialize model to JSON\n",
    "if not os.path.isfile(keras_model_json):\n",
    "    model_json = model.to_json()\n",
    "    with open(keras_model_json, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(keras_model_weights)\n",
    "\n",
    "with open(keras_model_json, \"r\") as keras_model_json_file:\n",
    "    keras_model_as_json = json.load(keras_model_json_file)\n",
    "\n",
    "keras_model = model_from_json(open(keras_model_json).read())\n",
    "keras_model.load_weights(keras_model_weights)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# tf.compat.v1.disable_v2_behavior()\n",
    "import sys\n",
    "\n",
    "shuffle_several_times = lambda s: np.array([onehot_dinuc_shuffle(s) for i in range(10)])\n",
    "seqs_to_explain = data_one_hot[0:3] #these three are positive for task 0\n",
    "\n",
    "sys.setrecursionlimit(9000)\n",
    "\n",
    "dinuc_shuff_explainer = shap.DeepExplainer((keras_model.input, keras_model.output[:,0]),\n",
    "                                           shuffle_several_times)\n",
    "raw_shap_explanations = dinuc_shuff_explainer.shap_values(seqs_to_explain)\n",
    "\n",
    "#project the importance at each position onto the base that's actually present\n",
    "shap_explanations = np.sum(raw_shap_explanations,axis=-1)[:,:,None]*seqs_to_explain\n",
    "\n",
    "for idx,(hypimpscores,orig_seq) in enumerate(zip(shap_explanations,seqs_to_explain)):\n",
    "    print(\"Sequence idx\",idx)\n",
    "    print(\"Actual contributions\")\n",
    "    # (The actual importance scores can be computed using an element-wise product of\n",
    "    #  the hypothetical importance scores and the actual importance scores)\n",
    "    viz_sequence.plot_weights(hypimpscores*orig_seq, subticks_frequency=20)\n",
    "    print(\"Hypothetical contributions\")\n",
    "    viz_sequence.plot_weights(hypimpscores, subticks_frequency=20)\n",
    "\n",
    "hypothetical_scores = shap_explanations\n",
    "original_scores = shap_explanations*seqs_to_explain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMPQdX4cOoZzqVG2F9xuy/Y",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
