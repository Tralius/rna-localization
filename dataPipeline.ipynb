{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Genomics Project **RNA Localisation**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Problem definition**:\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.** First steps with data\n",
    "\n",
    "Firstly, we import several necessary packages and load in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T13:33:04.336065Z",
     "start_time": "2023-06-13T13:32:51.261984Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 15:32:54.964375: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from keras.utils import to_categorical\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import StratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Import models\n",
    "import cnn_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T13:33:06.999309Z",
     "start_time": "2023-06-13T13:33:06.990425Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T13:33:17.000127Z",
     "start_time": "2023-06-13T13:33:09.364859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing test set\n",
    "np.random.seed(3)\n",
    "data_org = pd.read_csv('final_data.csv')\n",
    "test_data = data_org.sample(frac=0.1)\n",
    "train_data = data_org.drop(test_data.index)\n",
    "\n",
    "data_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T13:33:18.149377Z",
     "start_time": "2023-06-13T13:33:18.099539Z"
    }
   },
   "outputs": [],
   "source": [
    "sum_vec = data_org.iloc[:, :9].sum(axis=1)\n",
    "data2 = data_org.iloc[:, :9].divide(sum_vec, axis='index')\n",
    "data = pd.concat([data2, data_org['seq']], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T13:33:32.127319Z",
     "start_time": "2023-06-13T13:33:20.770702Z"
    }
   },
   "outputs": [],
   "source": [
    "# One hot encode the 'seq' attribute of the above table\n",
    "mapping = { \n",
    "            'A': 0,\n",
    "            'C': 1,\n",
    "            'G': 2,\n",
    "            'T': 3\n",
    "            }\n",
    "one_hot_encode_lam = lambda seq: to_categorical([mapping[x] for x in seq])\n",
    "data_one_hot_karas = data['seq'].apply(one_hot_encode_lam)\n",
    "\n",
    "data_one_hot_karas\n",
    "\n",
    "# Now just injecting this modified 'seq' back into the pandas frame \n",
    "data_new_karas =  pd.concat([data.iloc[:, :9], data_one_hot_karas], axis=1)\n",
    "\n",
    "data_new_karas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T13:33:36.566554Z",
     "start_time": "2023-06-13T13:33:36.472517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13810, 4, 64)\n",
      "(13810, 9)\n"
     ]
    }
   ],
   "source": [
    "# Splitting for 5fold\n",
    "\n",
    "folds_total = 5\n",
    "\n",
    "# Only Truncation?\n",
    "X = data_new_karas['seq'].apply(lambda x: x[:64])\n",
    "# X = data_new_karas['seq']\n",
    "\n",
    "# y = pd.DataFrame(columns=['Labels'])\n",
    "\n",
    "# X = tf.data.Dataset.from_tensor_slices(dict(X))\n",
    "\n",
    "y = data_new_karas.iloc[:, :9].values.tolist()\n",
    "# y = y.apply(lambda x: np.array(x))\n",
    "\n",
    "# y = tf.data.Dataset.from_tensor_slices(dict(y))\n",
    "\n",
    "X = np.stack(X).transpose((0, 2, 1))\n",
    "y = np.stack(y)\n",
    "\n",
    "# X = tf.convert_to_tensor(X)\n",
    "# y = tf.convert_to_tensor(y)\n",
    "\n",
    "# kf = KFold(n_splits=folds_total, shuffle=True, random_state=1234)\n",
    "\n",
    "# folds = kf.split(X, y)\n",
    "\n",
    "kf = KFold(n_splits=folds_total, shuffle=True, random_state=1)\n",
    "\n",
    "folds = kf.split(X,y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "# folds now contains a list of lists. Each sublist contains all the indices for the pandas data entries to be used in the respective fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: what is this?\n",
    "\n",
    "template = [0] * 24  # dim([a,c,g,t]) * dim([f,t,i,h,m,s])\n",
    "combined_encoding = OrderedDict()\n",
    "combined_encoding['UNK'] = template\n",
    "for i, (key_seq, key_ann) in enumerate(\n",
    "        itertools.product(['A', 'C', 'G', 'T', 'N'], ['F', 'T', 'I', 'H', 'M', 'S'])):\n",
    "    tmp = template.copy()\n",
    "    if key_seq == 'N':\n",
    "        for n in ['A', 'C', 'G', 'T']:\n",
    "            tmp[np.nonzero(combined_encoding[n + key_ann])[0][0]] = 0.25\n",
    "        combined_encoding[key_seq + key_ann] = tmp\n",
    "    else:\n",
    "        tmp[i] = 1  # normal one-hot encoding as it is...\n",
    "        combined_encoding[key_seq + key_ann] = tmp\n",
    "encoding_keys = list(combined_encoding.keys())\n",
    "encoding_vectors = np.array(list(combined_encoding.values()))\n",
    "encoding_vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2** |  Importing models...\n",
    "\n",
    "We use as baseline model the [RNATracker](https://github.com/HarveyYan/RNATracker/blob/master/Models/cnn_bilstm_attention.py) model and a CNN model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T13:33:44.925823Z",
     "start_time": "2023-06-13T13:33:42.487677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder already exists\n"
     ]
    }
   ],
   "source": [
    "# Import NN\n",
    "from RNAtracker import RNATracker\n",
    "\n",
    "# TODO: do a normal CNN\n",
    "\n",
    "\n",
    "# Set paths for model output\n",
    "try:\n",
    "    os.mkdir('model_outputs/')\n",
    "except Exception as e:\n",
    "    print(\"Output folder already exists\")\n",
    "\n",
    "model_output_folder = 'model_outputs/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T13:33:48.293206Z",
     "start_time": "2023-06-13T13:33:47.999445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 2, 32)             6176      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 1, 32)            0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1, 9)              297       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,473\n",
      "Trainable params: 6,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: with understand what we have to predict, we can allocate X and y\n",
    "# Also: the kwargsvalues are hyperparameters of which we will select default values from the RNAtracker repo\n",
    "\n",
    "VALIDATION_ACCURACY = []\n",
    "VALIDATION_LOSS = []\n",
    "\n",
    "print(\"Start training...\")\n",
    "\n",
    "for i, (train_indices, test_indices) in enumerate(folds):\n",
    "    print('Evaluating KFolds {}/{}'.format(str(i + 1), str(folds_total)))\n",
    "    # model = RNATracker(max_len, nb_classes, model_output_folder, kfold_index=i)\n",
    "    #model.build_model(nb_filters=kwargs['nb_filters'], filters_length=kwargs['filters_length'],\n",
    "    #                          pooling_size=kwargs['pooling_size'], lstm_units=kwargs['lstm_units'],\n",
    "    #                          embedding_vec=encoding_vectors)\n",
    "\n",
    "    # model.build_model_advanced_masking(nb_filters=kwargs['nb_filters'],\n",
    "     #                                              filters_length=kwargs['filters_length'],\n",
    "      #                                             pooling_size=kwargs['pooling_size'],\n",
    "       #                                            lstm_units=kwargs['lstm_units'],\n",
    "        #                                           embedding_vec=encoding_vectors)\n",
    "    \n",
    "\n",
    "    # model.train(X[train_indices], y[train_indices], batch_size, kwargs['epochs'])\n",
    "\n",
    "    # model.evaluate(X[test_indices], y[test_indices], dataset)\n",
    "\n",
    "    # K.clear_session()\n",
    "    model = cnn_model.CNN_MLRG().build_model()\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(model_output_folder + f\"cnn{i}.h5\",\n",
    "\t\t\t\t\t\t\tmonitor='val_accuracy', verbose=1,\n",
    "\t\t\t\t\t\t\tsave_best_only=True, mode='max')\n",
    "\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    history = model.fit(X[train_indices], y[train_indices], epochs=3,\n",
    "                        callbacks=callbacks_list, validation_data=(X[test_indices], y[test_indices]))\n",
    "\n",
    "    model.load_weights(f\"/model_outputs/cnn{i}.h5\")\n",
    "\n",
    "    results = model.evaluate(X[test_indices])\n",
    "    results = dict(zip(model.metrics_names,results))\n",
    "\n",
    "    VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "    VALIDATION_LOSS.append(results['loss'])\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"Training done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rna_localization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
