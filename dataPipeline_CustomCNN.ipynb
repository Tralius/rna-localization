{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Genomics Project **RNA Localisation**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Problem definition**:\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.** Data Preparation\n",
    "\n",
    "Firstly, we import several necessary packages and load in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-25T23:28:33.014534500Z",
     "start_time": "2023-06-25T23:28:33.012522600Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from notes.utils import read_model_file\n",
    "from models.utils import plot_line_graph, box_plot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-25T23:28:37.570448500Z",
     "start_time": "2023-06-25T23:28:34.605499500Z"
    }
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mParserError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 14\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m     13\u001B[0m     np\u001B[39m.\u001B[39mrandom\u001B[39m.\u001B[39mseed(\u001B[39m3\u001B[39m)\n\u001B[0;32m---> 14\u001B[0m     data_org \u001B[39m=\u001B[39m pd\u001B[39m.\u001B[39;49mread_csv(\u001B[39m'\u001B[39;49m\u001B[39m~/Downloads/final_data.csv\u001B[39;49m\u001B[39m'\u001B[39;49m)\n\u001B[1;32m     15\u001B[0m     test_data \u001B[39m=\u001B[39m data_org\u001B[39m.\u001B[39msample(frac\u001B[39m=\u001B[39m\u001B[39m0.1\u001B[39m)\n\u001B[1;32m     16\u001B[0m     train_data \u001B[39m=\u001B[39m data_org\u001B[39m.\u001B[39mdrop(test_data\u001B[39m.\u001B[39mindex) \u001B[39m# TODO: note: we also have to preprocess the test set similary\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/reggen/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m    899\u001B[0m kwds_defaults \u001B[39m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m    900\u001B[0m     dialect,\n\u001B[1;32m    901\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    908\u001B[0m     dtype_backend\u001B[39m=\u001B[39mdtype_backend,\n\u001B[1;32m    909\u001B[0m )\n\u001B[1;32m    910\u001B[0m kwds\u001B[39m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m--> 912\u001B[0m \u001B[39mreturn\u001B[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001B[0;32m~/anaconda3/envs/reggen/lib/python3.11/site-packages/pandas/io/parsers/readers.py:583\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    580\u001B[0m     \u001B[39mreturn\u001B[39;00m parser\n\u001B[1;32m    582\u001B[0m \u001B[39mwith\u001B[39;00m parser:\n\u001B[0;32m--> 583\u001B[0m     \u001B[39mreturn\u001B[39;00m parser\u001B[39m.\u001B[39;49mread(nrows)\n",
      "File \u001B[0;32m~/anaconda3/envs/reggen/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1704\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m   1697\u001B[0m nrows \u001B[39m=\u001B[39m validate_integer(\u001B[39m\"\u001B[39m\u001B[39mnrows\u001B[39m\u001B[39m\"\u001B[39m, nrows)\n\u001B[1;32m   1698\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m   1699\u001B[0m     \u001B[39m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[1;32m   1700\u001B[0m     (\n\u001B[1;32m   1701\u001B[0m         index,\n\u001B[1;32m   1702\u001B[0m         columns,\n\u001B[1;32m   1703\u001B[0m         col_dict,\n\u001B[0;32m-> 1704\u001B[0m     ) \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_engine\u001B[39m.\u001B[39;49mread(  \u001B[39m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[1;32m   1705\u001B[0m         nrows\n\u001B[1;32m   1706\u001B[0m     )\n\u001B[1;32m   1707\u001B[0m \u001B[39mexcept\u001B[39;00m \u001B[39mException\u001B[39;00m:\n\u001B[1;32m   1708\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/anaconda3/envs/reggen/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m    233\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mlow_memory:\n\u001B[0;32m--> 234\u001B[0m         chunks \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_reader\u001B[39m.\u001B[39;49mread_low_memory(nrows)\n\u001B[1;32m    235\u001B[0m         \u001B[39m# destructive to chunks\u001B[39;00m\n\u001B[1;32m    236\u001B[0m         data \u001B[39m=\u001B[39m _concatenate_chunks(chunks)\n",
      "File \u001B[0;32m~/anaconda3/envs/reggen/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:812\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/reggen/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:873\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/reggen/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:848\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/reggen/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:859\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/reggen/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:2025\u001B[0m, in \u001B[0;36mpandas._libs.parsers.raise_parser_error\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mParserError\u001B[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "# Initializing test/train split\n",
    "\n",
    "\n",
    "colab = False  #### Set colab flag ####\n",
    "\n",
    "if colab:\n",
    "    np.random.seed(3)\n",
    "    url = 'https://www.dropbox.com/s/hv4uau8q4wwg00k/final_data.csv?dl=1'\n",
    "    data_org = pd.read_csv(url)\n",
    "    test_data = data_org.sample(frac=0.1)\n",
    "    train_data = data_org.drop(test_data.index)\n",
    "else:\n",
    "    np.random.seed(3)\n",
    "    data_org = pd.read_csv('~/Downloads/final_data.csv')\n",
    "    test_data = data_org.sample(frac=0.1)\n",
    "    train_data = data_org.drop(test_data.index) # TODO: note: we also have to preprocess the test set similary\n",
    "    # TODO: colab\n",
    "\n",
    "data_org"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2** |  Initializations\n",
    "\n",
    "We use as baseline model the [RNATracker](https://github.com/HarveyYan/RNATracker/blob/master/Models/cnn_bilstm_attention.py) model and a CNN model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-25T23:28:39.003310600Z",
     "start_time": "2023-06-25T23:28:38.996300600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34526\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = train_data['seq'].apply(lambda x: len(x)).max()\n",
    "print(max_seq_len)\n",
    "# MODEL\n",
    "model_path = \"model_architectures/CNN_architecture4.yaml\"\n",
    "\n",
    "# Path where to save viz\n",
    "model_architecture_path = \"model_architecture_viz/simple_cnn.png\"\n",
    "\n",
    "params_dict = read_model_file(model_path, max_seq_len)\n",
    "param_dataLoader_valid = params_dict['param_dataLoader_valid']\n",
    "param_dataLoader_train = params_dict['param_dataLoader_train']\n",
    "params_model = params_dict['params_model']\n",
    "params_train = params_dict['params_train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-25T23:52:59.903071100Z",
     "start_time": "2023-06-25T23:28:42.486819600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 14:34:34.335092: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12/311 [>.............................] - ETA: 13:05 - loss: 124.8965 - accuracy: 0.1120 - kullback_leibler_divergence: 8.7094"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 17\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[39m# for i, (train_split, valid_split) in enumerate(folds):\u001B[39;00m\n\u001B[1;32m     12\u001B[0m model \u001B[39m=\u001B[39m CNN(\n\u001B[1;32m     13\u001B[0m             input_size\u001B[39m=\u001B[39m(max_seq_len, \u001B[39m4\u001B[39m),\n\u001B[1;32m     14\u001B[0m             params_model\u001B[39m=\u001B[39mparams_model)\n\u001B[0;32m---> 17\u001B[0m history \u001B[39m=\u001B[39m model\u001B[39m.\u001B[39;49mfit_and_evaluate(train_data\u001B[39m=\u001B[39;49mtrain_split, eval_data\u001B[39m=\u001B[39;49mvalid_split,\n\u001B[1;32m     18\u001B[0m                                  params_train_dataLoader\u001B[39m=\u001B[39;49mparam_dataLoader_train,\n\u001B[1;32m     19\u001B[0m                                  params_eval_dataLoader\u001B[39m=\u001B[39;49mparam_dataLoader_valid,\n\u001B[1;32m     20\u001B[0m                                  params_train\u001B[39m=\u001B[39;49mparams_train)\n\u001B[1;32m     22\u001B[0m \u001B[39m# results = model.evaluate(eval_data=valid_split, **param_dataLoader_valid)\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[39m# results = dict(zip(model.model.metrics_names, results))\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \n\u001B[1;32m     25\u001B[0m \u001B[39m# VALIDATION_ACCURACY.append(results['accuracy'])\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \u001B[39m# VALIDATION_LOSS.append(results['loss'])\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/main_storage/STUDY RESOURCES/Regulatory Genomics/project/files2/rna-localization/models/functional_CNN.py:85\u001B[0m, in \u001B[0;36mCNN.fit_and_evaluate\u001B[0;34m(self, train_data, eval_data, callback, params_train_dataLoader, params_eval_dataLoader, params_train)\u001B[0m\n\u001B[1;32m     81\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mfit_and_evaluate\u001B[39m(\u001B[39mself\u001B[39m, train_data, eval_data, callback: List[keras\u001B[39m.\u001B[39mcallbacks\u001B[39m.\u001B[39mCallback] \u001B[39m=\u001B[39m \u001B[39mNone\u001B[39;00m,\n\u001B[1;32m     82\u001B[0m                      params_train_dataLoader: Dict \u001B[39m=\u001B[39m \u001B[39mNone\u001B[39;00m,\n\u001B[1;32m     83\u001B[0m                      params_eval_dataLoader: Dict \u001B[39m=\u001B[39m \u001B[39mNone\u001B[39;00m,\n\u001B[1;32m     84\u001B[0m                      params_train: Dict \u001B[39m=\u001B[39m \u001B[39mNone\u001B[39;00m):\n\u001B[0;32m---> 85\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39msuper\u001B[39;49m()\u001B[39m.\u001B[39;49mfit_and_evaluate(train_data, eval_data, callback,\n\u001B[1;32m     86\u001B[0m                                     params_train_dataLoader,\n\u001B[1;32m     87\u001B[0m                                     params_eval_dataLoader,\n\u001B[1;32m     88\u001B[0m                                     params_train)\n",
      "File \u001B[0;32m~/Documents/main_storage/STUDY RESOURCES/Regulatory Genomics/project/files2/rna-localization/models/functional_model.py:55\u001B[0m, in \u001B[0;36mFunc_Model.fit_and_evaluate\u001B[0;34m(self, train_data, eval_data, callback, params_train_dataLoader, params_eval_dataLoader, params_train)\u001B[0m\n\u001B[1;32m     53\u001B[0m train_dataLoader \u001B[39m=\u001B[39m GeneDataLoader(train_data, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mparams_train_dataLoader)\n\u001B[1;32m     54\u001B[0m eval_dataLoader \u001B[39m=\u001B[39m GeneDataLoader(eval_data, shuffle\u001B[39m=\u001B[39m\u001B[39mFalse\u001B[39;00m,\u001B[39m*\u001B[39m\u001B[39m*\u001B[39mparams_eval_dataLoader)\n\u001B[0;32m---> 55\u001B[0m \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mmodel\u001B[39m.\u001B[39;49mfit(train_dataLoader, callbacks\u001B[39m=\u001B[39;49mcallback, validation_data\u001B[39m=\u001B[39;49meval_dataLoader, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mparams_train)\n",
      "File \u001B[0;32m~/anaconda3/envs/reggen/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[39m=\u001B[39m \u001B[39mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[39mreturn\u001B[39;00m fn(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m     66\u001B[0m \u001B[39mexcept\u001B[39;00m \u001B[39mException\u001B[39;00m \u001B[39mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[39m=\u001B[39m _process_traceback_frames(e\u001B[39m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/anaconda3/envs/reggen/lib/python3.11/site-packages/keras/engine/training.py:1685\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1677\u001B[0m \u001B[39mwith\u001B[39;00m tf\u001B[39m.\u001B[39mprofiler\u001B[39m.\u001B[39mexperimental\u001B[39m.\u001B[39mTrace(\n\u001B[1;32m   1678\u001B[0m     \u001B[39m\"\u001B[39m\u001B[39mtrain\u001B[39m\u001B[39m\"\u001B[39m,\n\u001B[1;32m   1679\u001B[0m     epoch_num\u001B[39m=\u001B[39mepoch,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1682\u001B[0m     _r\u001B[39m=\u001B[39m\u001B[39m1\u001B[39m,\n\u001B[1;32m   1683\u001B[0m ):\n\u001B[1;32m   1684\u001B[0m     callbacks\u001B[39m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m-> 1685\u001B[0m     tmp_logs \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mtrain_function(iterator)\n\u001B[1;32m   1686\u001B[0m     \u001B[39mif\u001B[39;00m data_handler\u001B[39m.\u001B[39mshould_sync:\n\u001B[1;32m   1687\u001B[0m         context\u001B[39m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m~/anaconda3/envs/reggen/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[39m=\u001B[39m \u001B[39mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[39mreturn\u001B[39;00m fn(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m    151\u001B[0m \u001B[39mexcept\u001B[39;00m \u001B[39mException\u001B[39;00m \u001B[39mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[39m=\u001B[39m _process_traceback_frames(e\u001B[39m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/anaconda3/envs/reggen/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    891\u001B[0m compiler \u001B[39m=\u001B[39m \u001B[39m\"\u001B[39m\u001B[39mxla\u001B[39m\u001B[39m\"\u001B[39m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_jit_compile \u001B[39melse\u001B[39;00m \u001B[39m\"\u001B[39m\u001B[39mnonXla\u001B[39m\u001B[39m\"\u001B[39m\n\u001B[1;32m    893\u001B[0m \u001B[39mwith\u001B[39;00m OptionalXlaContext(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 894\u001B[0m   result \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_call(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwds)\n\u001B[1;32m    896\u001B[0m new_tracing_count \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    897\u001B[0m without_tracing \u001B[39m=\u001B[39m (tracing_count \u001B[39m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/anaconda3/envs/reggen/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    923\u001B[0m   \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_lock\u001B[39m.\u001B[39mrelease()\n\u001B[1;32m    924\u001B[0m   \u001B[39m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    925\u001B[0m   \u001B[39m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 926\u001B[0m   \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_no_variable_creation_fn(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwds)  \u001B[39m# pylint: disable=not-callable\u001B[39;00m\n\u001B[1;32m    927\u001B[0m \u001B[39melif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_variable_creation_fn \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[1;32m    928\u001B[0m   \u001B[39m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    929\u001B[0m   \u001B[39m# in parallel.\u001B[39;00m\n\u001B[1;32m    930\u001B[0m   \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_lock\u001B[39m.\u001B[39mrelease()\n",
      "File \u001B[0;32m~/anaconda3/envs/reggen/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001B[0m, in \u001B[0;36mTracingCompiler.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[39mwith\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_lock:\n\u001B[1;32m    141\u001B[0m   (concrete_function,\n\u001B[1;32m    142\u001B[0m    filtered_flat_args) \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[0;32m--> 143\u001B[0m \u001B[39mreturn\u001B[39;00m concrete_function\u001B[39m.\u001B[39;49m_call_flat(\n\u001B[1;32m    144\u001B[0m     filtered_flat_args, captured_inputs\u001B[39m=\u001B[39;49mconcrete_function\u001B[39m.\u001B[39;49mcaptured_inputs)\n",
      "File \u001B[0;32m~/anaconda3/envs/reggen/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1753\u001B[0m possible_gradient_type \u001B[39m=\u001B[39m gradients_util\u001B[39m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1754\u001B[0m \u001B[39mif\u001B[39;00m (possible_gradient_type \u001B[39m==\u001B[39m gradients_util\u001B[39m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1755\u001B[0m     \u001B[39mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1756\u001B[0m   \u001B[39m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1757\u001B[0m   \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_build_call_outputs(\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_inference_function\u001B[39m.\u001B[39;49mcall(\n\u001B[1;32m   1758\u001B[0m       ctx, args, cancellation_manager\u001B[39m=\u001B[39;49mcancellation_manager))\n\u001B[1;32m   1759\u001B[0m forward_backward \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1760\u001B[0m     args,\n\u001B[1;32m   1761\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1762\u001B[0m     executing_eagerly)\n\u001B[1;32m   1763\u001B[0m forward_function, args_with_tangents \u001B[39m=\u001B[39m forward_backward\u001B[39m.\u001B[39mforward()\n",
      "File \u001B[0;32m~/anaconda3/envs/reggen/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    379\u001B[0m \u001B[39mwith\u001B[39;00m _InterpolateFunctionError(\u001B[39mself\u001B[39m):\n\u001B[1;32m    380\u001B[0m   \u001B[39mif\u001B[39;00m cancellation_manager \u001B[39mis\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[0;32m--> 381\u001B[0m     outputs \u001B[39m=\u001B[39m execute\u001B[39m.\u001B[39;49mexecute(\n\u001B[1;32m    382\u001B[0m         \u001B[39mstr\u001B[39;49m(\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49msignature\u001B[39m.\u001B[39;49mname),\n\u001B[1;32m    383\u001B[0m         num_outputs\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_num_outputs,\n\u001B[1;32m    384\u001B[0m         inputs\u001B[39m=\u001B[39;49margs,\n\u001B[1;32m    385\u001B[0m         attrs\u001B[39m=\u001B[39;49mattrs,\n\u001B[1;32m    386\u001B[0m         ctx\u001B[39m=\u001B[39;49mctx)\n\u001B[1;32m    387\u001B[0m   \u001B[39melse\u001B[39;00m:\n\u001B[1;32m    388\u001B[0m     outputs \u001B[39m=\u001B[39m execute\u001B[39m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m    389\u001B[0m         \u001B[39mstr\u001B[39m(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39msignature\u001B[39m.\u001B[39mname),\n\u001B[1;32m    390\u001B[0m         num_outputs\u001B[39m=\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_num_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    393\u001B[0m         ctx\u001B[39m=\u001B[39mctx,\n\u001B[1;32m    394\u001B[0m         cancellation_manager\u001B[39m=\u001B[39mcancellation_manager)\n",
      "File \u001B[0;32m~/anaconda3/envs/reggen/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m   ctx\u001B[39m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 52\u001B[0m   tensors \u001B[39m=\u001B[39m pywrap_tfe\u001B[39m.\u001B[39;49mTFE_Py_Execute(ctx\u001B[39m.\u001B[39;49m_handle, device_name, op_name,\n\u001B[1;32m     53\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[1;32m     54\u001B[0m \u001B[39mexcept\u001B[39;00m core\u001B[39m.\u001B[39m_NotOkStatusException \u001B[39mas\u001B[39;00m e:\n\u001B[1;32m     55\u001B[0m   \u001B[39mif\u001B[39;00m name \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# training, only need for the model initialization to change in general\n",
    "from models import CNN\n",
    "\n",
    "VALIDATION_ACCURACY = []\n",
    "VALIDATION_LOSS = []\n",
    "\n",
    "# 80/20 split\n",
    "train_split, valid_split = train_test_split(train_data, random_state=42, test_size=0.2)\n",
    "\n",
    "\n",
    "# for i, (train_split, valid_split) in enumerate(folds):\n",
    "model = CNN(\n",
    "            input_size=(max_seq_len, 4),\n",
    "            params_model=params_model,\n",
    "checkpoint_filepath = \"/checkpoints\")\n",
    "\n",
    "if checkpoint_filepath is None:\n",
    "    checkpoint_filepath = '/checkpoint'\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    save_freq = \"epoch\")\n",
    "\n",
    "history = model.fit_and_evaluate(train_data=train_split, eval_data=valid_split,\n",
    "                                 callback = [model_checkpoint_callback]\n",
    "                                 params_train_dataLoader=param_dataLoader_train,\n",
    "                                 params_eval_dataLoader=param_dataLoader_valid,\n",
    "                                 params_train=params_train)\n",
    "\n",
    "\n",
    "\n",
    "# results = model.evaluate(eval_data=valid_split, **param_dataLoader_valid)\n",
    "# results = dict(zip(model.model.metrics_names, results))\n",
    "\n",
    "# VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "# VALIDATION_LOSS.append(results['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T06:52:44.013176100Z",
     "start_time": "2023-06-26T06:52:42.084633500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.print_model(model_architecture_path)\n",
    "import datetime\n",
    "import pydot\n",
    "time_date = datetime.datetime.now().date()\n",
    "# time_hour = datetime.datetime.now().time()\n",
    "\n",
    "model_output = f\"model_outputs/simple_cnn_{time_date}.h5\"\n",
    "\n",
    "model.save_model(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T06:52:47.847776800Z",
     "start_time": "2023-06-26T06:52:47.488876300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 34526, 4)]   0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 34526, 16)    720         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 34526, 16)   64          ['conv1d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 34526, 16)    2832        ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 34526, 16)    0           ['conv1d[0][0]',                 \n",
      "                                                                  'conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 34526, 16)   64          ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 34526, 16)    0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 34526, 16)    0           ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 34523, 16)    0           ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 34523, 32)    5664        ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 34523, 32)   128         ['conv1d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 34523, 32)    11296       ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 34523, 32)    0           ['conv1d_2[0][0]',               \n",
      "                                                                  'conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 34523, 32)   128         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 34523, 32)    0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 34520, 32)   0           ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1104640)      0           ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 9)            9941769     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,962,665\n",
      "Trainable params: 9,962,473\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m a \u001B[39m=\u001B[39m model\u001B[39m.\u001B[39msummary()\n\u001B[1;32m      2\u001B[0m \u001B[39mprint\u001B[39m(a)\n\u001B[0;32m----> 4\u001B[0m plt_data \u001B[39m=\u001B[39m [history\u001B[39m.\u001B[39mhistory[\u001B[39m'\u001B[39m\u001B[39mloss\u001B[39m\u001B[39m'\u001B[39m], history\u001B[39m.\u001B[39mhistory[\u001B[39m'\u001B[39m\u001B[39mval_loss\u001B[39m\u001B[39m'\u001B[39m]]\n\u001B[1;32m      5\u001B[0m plot_line_graph(plt_data, \u001B[39m\"\u001B[39m\u001B[39mLoss Graph\u001B[39m\u001B[39m\"\u001B[39m, \u001B[39m'\u001B[39m\u001B[39mloss\u001B[39m\u001B[39m'\u001B[39m, \u001B[39m'\u001B[39m\u001B[39mepoch\u001B[39m\u001B[39m'\u001B[39m, [\u001B[39m'\u001B[39m\u001B[39mtrain\u001B[39m\u001B[39m'\u001B[39m, \u001B[39m'\u001B[39m\u001B[39mval\u001B[39m\u001B[39m'\u001B[39m])\n\u001B[1;32m      7\u001B[0m plt_data \u001B[39m=\u001B[39m [history\u001B[39m.\u001B[39mhistory[\u001B[39m'\u001B[39m\u001B[39maccuracy\u001B[39m\u001B[39m'\u001B[39m], history\u001B[39m.\u001B[39mhistory[\u001B[39m'\u001B[39m\u001B[39mval_accuracy\u001B[39m\u001B[39m'\u001B[39m]]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "a = model.summary()\n",
    "print(a)\n",
    "\n",
    "\n",
    "plt_data = [history.history['loss'], history.history['val_loss']]\n",
    "plot_line_graph(plt_data, \"Loss Graph\", 'loss', 'epoch', ['train', 'val'])\n",
    "\n",
    "plt_data = [history.history['accuracy'], history.history['val_accuracy']]\n",
    "plot_line_graph(plt_data, \"Accuracy Graph\", 'accuracy', 'epoch', ['train', 'val'])\n",
    "\n",
    "plt_data = [history.history['kullback_leibler_divergence'], history.history['val_kullback_leibler_divergence']]\n",
    "plot_line_graph(plt_data, \"kullback_leibler_divergence\", 'kullback_leibler_divergence', 'epoch', ['train', 'val'])\n",
    "\n",
    "plt_data = [history.history['tf_pearson'], history.history['val_tf_pearson']]\n",
    "plot_line_graph(plt_data, \"tf_pearson_Graph\", 'tf_pearson', 'epoch', ['train', 'val'])\n",
    "\n",
    "box_plot(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = model.evaluate(test_data, **param_dataLoader_valid)\n",
    "result = dict(zip(model.model.metrics_names, test_result))\n",
    "TEST_ACCURACY = result['accuracy']\n",
    "TEST_LOSS = result['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_LOSS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rna_localization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
